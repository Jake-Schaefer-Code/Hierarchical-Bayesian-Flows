{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixture Density Network (MDN)\n",
    "----\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_gaussians):\n",
    "        super(MDN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_gaussians * (2 * output_dim + 1))  # 2 for mean, variance and 1 for mixture weight\n",
    "        )\n",
    "        self.num_gaussians = num_gaussians\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        params = self.fc(x)\n",
    "        # Split the output into means, log variances, and mixture weights\n",
    "        means = params[:, :self.num_gaussians * self.output_dim].reshape(-1, self.num_gaussians, self.output_dim)\n",
    "        log_vars = params[:, self.num_gaussians * self.output_dim:2 * self.num_gaussians * self.output_dim]\n",
    "        log_vars = log_vars.reshape(-1, self.num_gaussians, self.output_dim)\n",
    "        mixture_weights = params[:, -self.num_gaussians:]\n",
    "        # Apply softmax for valid weights\n",
    "        mixture_weights = torch.softmax(mixture_weights, dim=1)\n",
    "        return means, log_vars, mixture_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mdn_loss(mixture_weights, means, log_vars, target):\n",
    "    \"\"\"\n",
    "    Is this literally just gaussian kde?\n",
    "    \"\"\"\n",
    "    gaussians = Normal(means, torch.exp(log_vars))  # Create Gaussian distributions\n",
    "    log_probs = gaussians.log_prob(target.unsqueeze(1).expand_as(means))  # Log likelihood of each Gaussian\n",
    "    log_probs = log_probs.sum(dim=2)  # Sum over the target dimensions\n",
    "    weighted_log_probs = log_probs + torch.log(mixture_weights)\n",
    "    log_sum_exp = torch.logsumexp(weighted_log_probs, dim=1)  # Log-sum-exp trick to prevent underflow\n",
    "    return -log_sum_exp.mean()\n",
    "\n",
    "# Data: Simulate parameter-data pairs (theta, x)\n",
    "# For simplicity, we simulate a 1D parameter theta and 1D observed data x\n",
    "def generate_data(num_samples=1000):\n",
    "    theta = torch.randn(num_samples, 1)  # Simulate theta ~ N(0, 1)\n",
    "    x = theta + 0.1 * torch.randn(num_samples, 1)  # Simulate data: x = theta + noise\n",
    "    return x, theta\n",
    "\n",
    "# Train the MDN\n",
    "def train_mdn(mdn, x_train, theta_train, epochs=1000, lr=0.001):\n",
    "    optimizer = optim.Adam(mdn.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        means, log_vars, mixture_weights = mdn(x_train)\n",
    "        loss = mdn_loss(mixture_weights, means, log_vars, theta_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "def posterior_estimation(mdn, x_observed):\n",
    "    with torch.no_grad():\n",
    "        means, log_vars, mixture_weights = mdn(x_observed)\n",
    "        posterior = Normal(means, torch.exp(log_vars))\n",
    "        return posterior, mixture_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.4296963214874268\n",
      "Epoch 100, Loss: -0.9065782427787781\n",
      "Epoch 200, Loss: -0.9227863550186157\n",
      "Epoch 300, Loss: -0.931168794631958\n",
      "Epoch 400, Loss: -0.943693995475769\n",
      "Epoch 500, Loss: -0.9545442461967468\n",
      "Epoch 600, Loss: -0.9415557384490967\n",
      "Epoch 700, Loss: -0.9531027674674988\n",
      "Epoch 800, Loss: -0.9710179567337036\n",
      "Epoch 900, Loss: -0.9669299125671387\n",
      "Posterior means: tensor([[[0.3181],\n",
      "         [0.5272],\n",
      "         [0.6455],\n",
      "         [0.4514],\n",
      "         [0.5808]]])\n",
      "Posterior std: tensor([[[0.0330],\n",
      "         [0.0542],\n",
      "         [0.0099],\n",
      "         [0.0511],\n",
      "         [0.0806]]])\n",
      "Mixture weights: tensor([[0.0806, 0.3058, 0.0257, 0.3464, 0.2415]])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_dim = 1  # Dimension of observed data x\n",
    "output_dim = 1  # Dimension of parameter theta\n",
    "num_gaussians = 5  # Number of Gaussian mixtures\n",
    "\n",
    "mdn = MDN(input_dim, output_dim, num_gaussians)\n",
    "\n",
    "# Generate training data\n",
    "x_train, theta_train = generate_data(num_samples=1000)\n",
    "\n",
    "# Train the MDN to learn the posterior\n",
    "train_mdn(mdn, x_train, theta_train, epochs=1000)\n",
    "\n",
    "# Estimate the posterior for a new observed data point\n",
    "x_observed = torch.tensor([[0.5]])  # Example observed data\n",
    "posterior, mixture_weights = posterior_estimation(mdn, x_observed)\n",
    "print(f\"Posterior means: {posterior.mean}\")\n",
    "print(f\"Posterior std: {posterior.stddev}\")\n",
    "print(f\"Mixture weights: {mixture_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jake_env312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
